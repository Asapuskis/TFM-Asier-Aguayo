{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Leeme!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!: </h1>\n",
    "\n",
    "<h3>Content Based</h3>\n",
    "\n",
    "<p>Hola Puskis del futuro, antes de irte a Italia dejaste así el TFM, te pongo en situación para que no tope de flipes.</p>\n",
    "<p>Hasta la parte de Generación del dataset con los valores normalizados, está todo bien, se han sacado los valores númericos de todas las variables categóricas y están almacenadas en dfJoined. También se ha generado el archivo pruebaPivoted2 con el json del dataframe.</p>\n",
    "<p>El problema es que no se puede usar dfJoined porque la memoria no da, estará mal distribuido y no hay espacio, por eso opté por sacar los datos a un json.</p>\n",
    "<p>El problema es que al cargar el json, las columnas con los valores de HashingTF-ID no se cargan bien. Has creado aquí abajo el schema a mano pero no consigues cargarlo correctamente. Está complicado inferir el schema, no encontraste como hacerlo automáticamente, yo flipo, igual buscar una librería de json externa para cargar el fichero? se me acaba de ocurrir.</p>\n",
    "<p>Lo bueno es que probaste con las demás columnas y generaba los valores normalizados correctamente, así que es cuestión de meter las dos columnas que faltan y generar un csv con el master_id, title, scaledFeatures. Con ese csv, cargarlo en X - Calculing similarity y ya estaría.</p>\n",
    "<p>Hay 2 opciones, hacer que cargue el json como dios manda y pasarle el pipeline o de alguna forma volcar el contenido de dfJoined a un dataframe nuevo para que no pete la memoria...</p>\n",
    "\n",
    "<h3>Collaborative Based</h3>\n",
    "\n",
    "<p>Parece bastante sencillo, recuerda el email que te mandó Miguel Angel de como combinar algoritmos con Surprise. No te líes. Lo único, habrá que Scrappear más usuarios y más ratings.</p>\n",
    "<p>Para Scrappear, en el cmd colocarse en la carpeta raíz del Scrappy: U:\\Master Data Science\\TFM\\Notebooks\\0 - discogsScrapy y ejecutar \"scrappy crawl usernameSpider -o nombreFichero.json\", esto generará un fichero en la carpeta en la que estamos. Nos interesa coger más usuarios, así que cogemos más páginas del foro, abre usernameSpider.py y añade más páginas.</p>\n",
    "<p>Después de scrappear, ejecuta el notebook 1 para coger los ratings de los usuarios y generar un archivo.</p>\n",
    "\n",
    "<p>Eso es todo, espero que hayas disfrutado de la bella Italia, un misu.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
